{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'UNet_lr_dynamic_batch_2_standard_scale_[1.0, 1.1]_colour_0.25_blur_0.03_linearup_weightedloss_wd_1e-7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import json  # To save and read mean and std values as JSON\n",
    "\n",
    "# Directory containing your training images\n",
    "image_directory = 'train/train_image'\n",
    "\n",
    "# Transform to convert the images to a tensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Path to store the mean and std\n",
    "stats_file_path = 'mean_std.json'\n",
    "\n",
    "# Function to calculate mean and std\n",
    "def calculate_mean_std():\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    num_pixels = 0\n",
    "\n",
    "    # Iterate through each image in the directory\n",
    "    for image_name in tqdm(os.listdir(image_directory)):\n",
    "        image_path = os.path.join(image_directory, image_name)\n",
    "        image = Image.open(image_path).convert('RGB')  # Ensure the image is in RGB format\n",
    "        image_tensor = transform(image)\n",
    "        \n",
    "        # Accumulate sum and sum of squares for mean and std calculation\n",
    "        mean += image_tensor.mean(dim=(1, 2))\n",
    "        std += image_tensor.std(dim=(1, 2))\n",
    "        num_pixels += 1\n",
    "\n",
    "    # Compute the mean and std for each channel\n",
    "    mean /= num_pixels\n",
    "    std /= num_pixels\n",
    "\n",
    "    # Convert to standard Python float types for saving to JSON\n",
    "    mean_list = mean.tolist()\n",
    "    std_list = std.tolist()\n",
    "\n",
    "    # Save to a file\n",
    "    stats = {'mean': mean_list, 'std': std_list}\n",
    "    with open(stats_file_path, 'w') as f:\n",
    "        json.dump(stats, f)\n",
    "    print(f'Saved mean and std to {stats_file_path}')\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "# Check if mean and std are already saved in the file\n",
    "if os.path.exists(stats_file_path):\n",
    "    print(f'{stats_file_path} found, loading mean and std...')\n",
    "    with open(stats_file_path, 'r') as f:\n",
    "        stats = json.load(f)\n",
    "        mean = torch.tensor(stats['mean'])\n",
    "        std = torch.tensor(stats['std'])\n",
    "    print(f'Mean: {mean}')\n",
    "    print(f'Std: {std}')\n",
    "else:\n",
    "    print(f'{stats_file_path} not found, calculating mean and std...')\n",
    "    mean, std = calculate_mean_std()\n",
    "\n",
    "print(f'Final Mean: {mean}')\n",
    "print(f'Final Std: {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed = 42 # This is the answer to the ultimate question of life, the universe and everything\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F1\n",
    "from torchvision import transforms\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class JointTransform:\n",
    "    def __init__( \n",
    "        self, resize=(512, 512), rotation_degree=15, scale=(0.9, 1.1), \n",
    "        brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, gaussian_noise_std=0.01,\n",
    "        apply_gaussian_blur=False, blur_kernel_size=3, mean=(0.4914, 0.4822, 0.4465), \n",
    "        std=(0.2023, 0.1994, 0.2010)\n",
    "    ):\n",
    "        self.resize = resize\n",
    "        self.rotation_degree = rotation_degree\n",
    "        self.scale = scale\n",
    "        self.brightness = brightness\n",
    "        self.contrast = contrast\n",
    "        self.saturation = saturation\n",
    "        self.hue = hue\n",
    "        self.gaussian_noise_std = gaussian_noise_std\n",
    "        self.apply_gaussian_blur = apply_gaussian_blur\n",
    "        self.blur_kernel_size = blur_kernel_size\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        # Resize\n",
    "        image = F1.resize(image, self.resize)\n",
    "        mask = F1.resize(mask, self.resize, interpolation=Image.NEAREST)\n",
    "\n",
    "    \n",
    "        # # Random Rotation (Only if rotation_degree > 0)\n",
    "        # if self.rotation_degree > 0:\n",
    "        #     angle = random.uniform(-self.rotation_degree, self.rotation_degree)\n",
    "        #     image = F1.rotate(image, angle)\n",
    "        #     mask = F1.rotate(mask, angle)\n",
    "        \n",
    "        # Random Scaling\n",
    "        scale_factor = random.uniform(self.scale[0], self.scale[1])\n",
    "        new_size = [int(self.resize[0] * scale_factor), int(self.resize[1] * scale_factor)]\n",
    "        image = F1.resize(image, new_size)\n",
    "        mask = F1.resize(mask, new_size, interpolation=Image.NEAREST)\n",
    "        \n",
    "        # Center Crop to Original Size\n",
    "        image = F1.center_crop(image, self.resize)\n",
    "        mask = F1.center_crop(mask, self.resize)\n",
    "        \n",
    "        # Color Jitter\n",
    "        color_jitter = transforms.ColorJitter(\n",
    "            brightness=self.brightness, \n",
    "            contrast=self.contrast, \n",
    "            saturation=self.saturation, \n",
    "            hue=self.hue\n",
    "        )\n",
    "        image = color_jitter(image)\n",
    "        \n",
    "        # Gaussian Blur\n",
    "        if self.apply_gaussian_blur and random.random() < 0.5:\n",
    "            image = F1.gaussian_blur(image, kernel_size=self.blur_kernel_size)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        image = F1.to_tensor(image)\n",
    "        mask = F1.pil_to_tensor(mask).squeeze(0).long()\n",
    "        \n",
    "        # Add Gaussian Noise\n",
    "        if self.gaussian_noise_std > 0:\n",
    "            noise = torch.randn(image.size()) * self.gaussian_noise_std\n",
    "            image = image + noise\n",
    "            \n",
    "        # Clamp image values to [0, 1] after adding noise\n",
    "        image = torch.clamp(image, 0.0, 1.0)\n",
    "        \n",
    "        # Normalize the image\n",
    "        image = F1.normalize(image, mean=self.mean, std=self.std)\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "train_transform = JointTransform(\n",
    "    resize=(512, 512), rotation_degree=0, scale=(1.0, 1.1),\n",
    "    brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1,\n",
    "    gaussian_noise_std=0.03, apply_gaussian_blur=True, blur_kernel_size=3,\n",
    "    mean=mean, std=std\n",
    ")\n",
    "val_transform = JointTransform(\n",
    "    resize=(512, 512), rotation_degree=0, scale=(1.0, 1.0),\n",
    "    brightness=0.0, contrast=0.0, saturation=0.0, hue=0.0,\n",
    "    gaussian_noise_std=0.0, apply_gaussian_blur=False, blur_kernel_size=0,\n",
    "    mean=mean, std=std\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as F1\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class CelebAMaskHQDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, image_ext='jpg', mask_ext='png', transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = [file for file in os.listdir(image_dir) if file.endswith(f'.{image_ext}')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[idx].replace('.jpg', '.png'))\n",
    "\n",
    "        # Load image and mask\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image, mask = self.transform(image, mask)\n",
    "        else:\n",
    "            image = F1.to_tensor(image)\n",
    "            mask = F1.pil_to_tensor(mask).squeeze(0).long()\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# Load training dataset with augmentation\n",
    "train_dataset = CelebAMaskHQDataset(\n",
    "    image_dir='train/train_image', \n",
    "    mask_dir='train/train_mask', \n",
    "    image_ext='jpg', mask_ext='png', \n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "# # Load training dataset with augmentation TINY\n",
    "# train_dataset = CelebAMaskHQDataset(\n",
    "#     image_dir='train_tiny/train_image', \n",
    "#     mask_dir='train_tiny/train_mask', \n",
    "#     image_ext='jpg', mask_ext='png', \n",
    "#     transform=train_transform\n",
    "# )\n",
    "\n",
    "# Load validation dataset without augmentation\n",
    "val_dataset = CelebAMaskHQDataset(\n",
    "    image_dir='val/val_image', \n",
    "    mask_dir='val/val_mask', \n",
    "    image_ext='jpg', mask_ext='png', \n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "# Create DataLoaders for train and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_dataset(loader, num_examples=5):\n",
    "    # Get one batch of data\n",
    "    dataiter = iter(loader)\n",
    "    images, masks = next(dataiter)\n",
    "    \n",
    "    # Ensure the number of examples does not exceed the batch size\n",
    "    num_examples = min(num_examples, images.size(0))\n",
    "    \n",
    "    # Convert tensors to NumPy arrays for plotting\n",
    "    images = images.numpy()\n",
    "    masks = masks.numpy()\n",
    "    \n",
    "    for idx in range(num_examples):\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        \n",
    "        # Image\n",
    "        img = images[idx].transpose(1, 2, 0)  # (C, H, W) to (H, W, C)\n",
    "        img = np.clip(img, 0, 1)  # Ensure values are between 0 and 1\n",
    "        ax[0].imshow(img)\n",
    "        ax[0].set_title('Image')\n",
    "        ax[0].axis('off')\n",
    "        \n",
    "        # Mask\n",
    "        mask = masks[idx]\n",
    "        ax[1].imshow(mask, cmap='jet', interpolation='nearest')\n",
    "        ax[1].set_title('Mask')\n",
    "        ax[1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training examples:\")\n",
    "visualize_dataset(train_loader, num_examples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation examples:\")\n",
    "visualize_dataset(val_loader, num_examples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "\n",
    "class SmallerUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallerUNet, self).__init__()\n",
    "\n",
    "        # Encoder: Convolution + ReLU + MaxPool (downsampling)\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # 512x512 -> 256x256\n",
    "        \n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # 256x256 -> 128x128\n",
    "        \n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # 128x128 -> 64x64\n",
    "        \n",
    "        self.encoder4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)  # 64x64 -> 32x32\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Decoder: Transpose Convolution + ReLU (upsampling)\n",
    "        self.upconv4 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)  # 32x32 -> 64x64\n",
    "        self.decoder4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)  # 64x64 -> 128x128\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)  # 128x128 -> 256x256\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)  # 256x256 -> 512x512\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Conv2d(16, 19, kernel_size=1)  # 1x1 Conv to map to the number of classes (19 in this case)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        enc1 = self.encoder1(x)\n",
    "        x = self.pool1(enc1)\n",
    "        \n",
    "        enc2 = self.encoder2(x)\n",
    "        x = self.pool2(enc2)\n",
    "        \n",
    "        enc3 = self.encoder3(x)\n",
    "        x = self.pool3(enc3)\n",
    "        \n",
    "        enc4 = self.encoder4(x)\n",
    "        x = self.pool4(enc4)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        # Decoder path with skip connections\n",
    "        x = self.upconv4(x)\n",
    "        x = torch.cat((enc4, x), dim=1)  # Concatenate encoder output (skip connection)\n",
    "        x = self.decoder4(x)\n",
    "\n",
    "        x = self.upconv3(x)\n",
    "        x = torch.cat((enc3, x), dim=1)\n",
    "        x = self.decoder3(x)\n",
    "\n",
    "        x = self.upconv2(x)\n",
    "        x = torch.cat((enc2, x), dim=1)\n",
    "        x = self.decoder2(x)\n",
    "\n",
    "        x = self.upconv1(x)\n",
    "        x = torch.cat((enc1, x), dim=1)\n",
    "        x = self.decoder1(x)\n",
    "\n",
    "        # Output layer\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # Get the number of GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "\n",
    "    # Print each GPU's index and name\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "num_classes = 19  # Replace with the number of classes in your dataset\n",
    "\n",
    "# Specify a specific GPU (e.g., GPU 2)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the smaller model\n",
    "model = SmallerUNet()\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Print model summary using torchsummary (input size: 3x512x512 for RGB images)\n",
    "summary(model, input_size=(3, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in the model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# File to save/load class distribution\n",
    "class_dist_file_path = 'class_distribution.json'\n",
    "\n",
    "# Function to compute class distribution\n",
    "def calculate_class_distribution(train_loader, num_classes=19):\n",
    "    # Initialize an empty tensor to hold the count of pixels for each of the classes\n",
    "    class_distribution = torch.zeros(num_classes, dtype=torch.long)\n",
    "\n",
    "    # Iterate through all batches in the train loader\n",
    "    for images, masks in tqdm(train_loader):  # Assuming masks are (batch_size, H, W) with integer class labels\n",
    "        # Flatten masks to count pixels across all batches and images\n",
    "        flattened_masks = masks.view(-1)  # (batch_size * H * W,)\n",
    "        \n",
    "        # Update class distribution by counting occurrences of each class (0 to num_classes-1)\n",
    "        for class_idx in range(num_classes):\n",
    "            class_distribution[class_idx] += (flattened_masks == class_idx).sum().item()\n",
    "\n",
    "    # Convert to a list for saving\n",
    "    class_distribution_list = class_distribution.tolist()\n",
    "\n",
    "    # Save the class distribution to a JSON file\n",
    "    with open(class_dist_file_path, 'w') as f:\n",
    "        json.dump({'class_distribution': class_distribution_list}, f)\n",
    "\n",
    "    print(f'Saved class distribution to {class_dist_file_path}')\n",
    "\n",
    "    return class_distribution\n",
    "\n",
    "# Check if class distribution file already exists\n",
    "if os.path.exists(class_dist_file_path):\n",
    "    print(f'{class_dist_file_path} found, loading class distribution...')\n",
    "    with open(class_dist_file_path, 'r') as f:\n",
    "        class_dist_data = json.load(f)\n",
    "        class_distribution = torch.tensor(class_dist_data['class_distribution'], dtype=torch.long)\n",
    "    print(f'Loaded class distribution: {class_distribution}')\n",
    "else:\n",
    "    print(f'{class_dist_file_path} not found, calculating class distribution...')\n",
    "    class_distribution = calculate_class_distribution(train_loader, num_classes=19)\n",
    "\n",
    "# Print out the pixel distribution across the 19 classes\n",
    "for class_idx in range(19):\n",
    "    print(f\"Class {class_idx}: {class_distribution[class_idx]} pixels\")\n",
    "\n",
    "# Now we compute the class weights dynamically based on class_distribution\n",
    "total_pixels = class_distribution.sum().item()  # Total number of pixels across all classes\n",
    "\n",
    "# Calculate class frequencies\n",
    "class_frequencies = class_distribution.float()\n",
    "\n",
    "# Add a small epsilon to avoid taking the log of zero\n",
    "epsilon = 1e-6\n",
    "class_weights = 1.0 / torch.log(class_frequencies + epsilon)\n",
    "\n",
    "# Normalise the weights to sum to 1 (optional)\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "# Print the computed class weights\n",
    "for class_idx in range(19):\n",
    "    print(f\"Weight for Class {class_idx}: {class_weights[class_idx].item()}\")\n",
    "\n",
    "# Assuming you are using CUDA, move the class weights to the device\n",
    "class_weights = class_weights.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Convert class distribution to frequencies (assuming class_distribution is defined)\n",
    "class_frequencies = class_distribution.float()\n",
    "\n",
    "# Create a plot\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "\n",
    "# Plot using seaborn's barplot for better aesthetics\n",
    "sns.barplot(x=np.arange(19), y=class_frequencies)\n",
    "\n",
    "# Set y-axis to log10 scale\n",
    "plt.yscale('log')\n",
    "\n",
    "# Beautify the plot\n",
    "plt.xlabel('Class Index', fontsize=30)\n",
    "plt.ylabel('Number of Pixels in Training Set', fontsize=30)\n",
    "\n",
    "# Add grid and style\n",
    "plt.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "plt.xticks(np.arange(19))\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot with ppi=300\n",
    "plt.savefig('class_frequency_distribution.png', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ExponentialLR\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "# Set up the log file path\n",
    "log_file = model_name + 'training_output.log'\n",
    "\n",
    "# If the file exists, delete it\n",
    "if os.path.exists(log_file):\n",
    "    os.remove(log_file)\n",
    "\n",
    "# Set up logging to save the output to a new .txt file\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "def log_message(message):\n",
    "    print(message)  # Print the message to console\n",
    "    logging.info(message)  # Save the message to the log file\n",
    "\n",
    "def calculate_intersect_union(pred_mask, gt_mask, num_classes=19):\n",
    "    area_intersect_all = np.zeros(num_classes)\n",
    "    area_union_all = np.zeros(num_classes)\n",
    "    \n",
    "    for cls_idx in range(num_classes):\n",
    "        area_intersect = np.sum((pred_mask == cls_idx) & (gt_mask == cls_idx))\n",
    "        area_pred_label = np.sum(pred_mask == cls_idx)\n",
    "        area_gt_label = np.sum(gt_mask == cls_idx)\n",
    "        area_union = area_pred_label + area_gt_label - area_intersect\n",
    "\n",
    "        area_intersect_all[cls_idx] += area_intersect\n",
    "        area_union_all[cls_idx] += area_union\n",
    "\n",
    "    return area_intersect_all, area_union_all\n",
    "\n",
    "# Combined training function with dynamic learning rate switching, patience handling, and mIoU calculation\n",
    "def train_dynamic(model, device, train_loader, val_loader, class_weights, initial_lr=0.001, min_sgd_lr=0.001, weight_decay=1e-6, scheduler_type='cosine', epochs=100, patience=10, checkpoint_path='best_model.pth', csv_file_name='losses.csv', load_weight=False, weight_path=None):\n",
    "    # Define the loss function\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)  # Multi-class segmentation loss\n",
    "\n",
    "    # Start with Adam optimiser\n",
    "    optimizer = optim.Adam(model.parameters(), lr=initial_lr, weight_decay=weight_decay)\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    sgd_lr = 0.02  # Initial learning rate for SGD phase\n",
    "\n",
    "        # Initialize scheduler\n",
    "    if scheduler_type == 'cosine':\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=0.5 * initial_lr)\n",
    "    elif scheduler_type == 'exponential':\n",
    "        scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    # Load pre-trained weights if specified\n",
    "    if load_weight and weight_path:\n",
    "        model.load_state_dict(torch.load(weight_path))\n",
    "        log_message(f\"Loaded weights from {weight_path}\")\n",
    "\n",
    "    # Lists to store train and validation losses and mIoU for each epoch\n",
    "    train_losses, val_losses = [], []\n",
    "    train_miou_list, val_miou_list = [], []\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        \n",
    "        # Initialize total intersection and union areas for the epoch\n",
    "        total_area_intersect_train = np.zeros(19)\n",
    "        total_area_union_train = np.zeros(19)\n",
    "\n",
    "        # Training loop\n",
    "        for images, masks in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update train loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Compute intersection and union for mIoU\n",
    "            pred_masks = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            gt_masks = masks.cpu().numpy()\n",
    "            \n",
    "            for batch_idx in range(images.shape[0]):\n",
    "                area_intersect, area_union = calculate_intersect_union(\n",
    "                    pred_masks[batch_idx], gt_masks[batch_idx]\n",
    "                )\n",
    "                total_area_intersect_train += area_intersect\n",
    "                total_area_union_train += area_union\n",
    "\n",
    "        # Compute IoU for each class\n",
    "        iou_per_class_train = total_area_intersect_train / (total_area_union_train + 1e-6)\n",
    "\n",
    "        # Identify valid classes (those with at least one pixel in the union)\n",
    "        valid_classes_train = total_area_union_train > 0\n",
    "\n",
    "        # Compute mean IoU over valid classes\n",
    "        train_miou = iou_per_class_train[valid_classes_train].mean() * 100\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        # Initialize total intersection and union areas for validation\n",
    "        total_area_intersect_val = np.zeros(19)\n",
    "        total_area_union_val = np.zeros(19)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Compute mIoU for validation\n",
    "                pred_masks = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "                gt_masks = masks.cpu().numpy()\n",
    "                \n",
    "                for batch_idx in range(images.shape[0]):\n",
    "                    area_intersect, area_union = calculate_intersect_union(\n",
    "                        pred_masks[batch_idx], gt_masks[batch_idx]\n",
    "                    )\n",
    "                    total_area_intersect_val += area_intersect\n",
    "                    total_area_union_val += area_union\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Save losses for plotting\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Compute IoU for each class\n",
    "        iou_per_class_val = total_area_intersect_val / (total_area_union_val + 1e-6)\n",
    "\n",
    "        # Identify valid classes (those with at least one pixel in the union)\n",
    "        valid_classes_val = total_area_union_val > 0\n",
    "\n",
    "        # Compute mean IoU over valid classes\n",
    "        val_miou = iou_per_class_val[valid_classes_val].mean() * 100\n",
    "\n",
    "        train_miou_list.append(train_miou)\n",
    "        val_miou_list.append(val_miou)\n",
    "\n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        log_message(f\"Epoch {epoch+1}/{epochs}, LR: {current_lr:.6f}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train mIoU: {train_miou:.2f}%, Val mIoU: {val_miou:.2f}%\")\n",
    "\n",
    "        # Update the learning rate using the scheduler at the end of each epoch\n",
    "        scheduler.step()\n",
    "\n",
    "        # Check for improvement in validation loss and save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0  # Reset patience counter\n",
    "            # Save model checkpoint\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            log_message(f\"Model saved at epoch {epoch+1}, with val loss: {val_loss:.4f}, val mIoU: {val_miou:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            log_message(f\"Patience counter: {patience_counter}/{patience}\")\n",
    "\n",
    "        # Early stopping and optimiser switch\n",
    "        if patience_counter >= patience:\n",
    "            log_message(f\"Patience reached at epoch {epoch+1}, switching optimiser...\")\n",
    "\n",
    "            # Load the best model checkpoint before switching the optimiser\n",
    "            model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "            # Switch to SGD optimiser with decreasing learning rate\n",
    "            if sgd_lr >= min_sgd_lr:\n",
    "                optimizer = optim.SGD(model.parameters(), lr=sgd_lr, weight_decay=weight_decay, momentum=0.5)\n",
    "                log_message(f\"Switched to SGD optimiser with learning rate: {sgd_lr:.4f}\")\n",
    "\n",
    "                # Reset the scheduler after switching the optimizer\n",
    "                if scheduler_type == 'cosine':\n",
    "                    scheduler = CosineAnnealingLR(optimizer, T_max=25, eta_min= 0.5 * sgd_lr)\n",
    "                elif scheduler_type == 'exponential':\n",
    "                    scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "        \n",
    "                log_message(f\"Scheduler reset after switching to SGD optimizer.\")\n",
    "                \n",
    "                sgd_lr /= 2  # Halve the learning rate for the next phase\n",
    "                patience_counter = 0  # Reset patience counter\n",
    "\n",
    "                \n",
    "            else:\n",
    "                log_message(\"SGD learning rate has dropped below the minimum threshold, stopping training.\")\n",
    "                break  # Stop training if learning rate drops below minimum threshold\n",
    "\n",
    "    # Load the best model weights after training\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    log_message(f\"Training finished. Best model loaded with val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Save train/val losses and mIoU to a CSV file\n",
    "    loss_df = pd.DataFrame({\n",
    "        'Epoch': range(1, len(train_losses) + 1),\n",
    "        'Train Loss': train_losses,\n",
    "        'Val Loss': val_losses,\n",
    "        'Train mIoU (%)': train_miou_list,\n",
    "        'Val mIoU (%)': val_miou_list\n",
    "    })\n",
    "    loss_df.to_csv(csv_file_name, index=False)\n",
    "    log_message(f\"Losses and mIoU saved to {csv_file_name}\")\n",
    "\n",
    "    # Plotting the train and validation loss and mIoU curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Val Loss')\n",
    "    plt.plot(range(1, len(train_miou_list) + 1), train_miou_list, label='Train mIoU (%)')\n",
    "    plt.plot(range(1, len(val_miou_list) + 1), val_miou_list, label='Val mIoU (%)')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss / mIoU (%)')\n",
    "    plt.title('Training and Validation Loss & mIoU Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the figure\n",
    "    plot_file_name = csv_file_name.replace('.csv', '.png')\n",
    "    plt.savefig(plot_file_name)\n",
    "    log_message(f\"Loss and mIoU curve figure saved to {plot_file_name}\")\n",
    "    plt.show()\n",
    "\n",
    "train_dynamic(model, device, train_loader, val_loader, class_weights, initial_lr=0.001, min_sgd_lr=0.005, weight_decay=1e-7, scheduler_type='cosine', epochs=120, patience=12, checkpoint_path=model_name + '_best.pth', csv_file_name=model_name + '_losses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_name+'_best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_predictions(model, val_loader, device, image_folder='val/val_image', num_images=4):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    images, masks = next(iter(val_loader))\n",
    "    images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "\n",
    "    # Ensure we do not exceed the number of images in the batch\n",
    "    num_images = min(num_images, images.size(0))\n",
    "\n",
    "    fig, axes = plt.subplots(num_images, 3, figsize=(15, num_images * 5))\n",
    "\n",
    "    if num_images == 1:\n",
    "        axes = [axes]  # Make sure axes is iterable when there's only 1 sample\n",
    "\n",
    "    for i in range(num_images):\n",
    "        # Load the original image from the folder (assuming filenames match the order in the val_loader)\n",
    "        image_name = os.listdir(image_folder)[i]  # Get the ith image name from the folder\n",
    "        image_path = os.path.join(image_folder, image_name)\n",
    "        original_image = Image.open(image_path)\n",
    "\n",
    "        # Show original image\n",
    "        axes[i][0].imshow(original_image)  \n",
    "        axes[i][0].set_title('Input Image', fontsize=30)  # Larger font size (2x larger)\n",
    "        axes[i][0].axis('off')  # Turn off axis for the input image\n",
    "\n",
    "        # Show ground truth mask\n",
    "        axes[i][1].imshow(masks[i].cpu(), cmap='tab20', vmin=0, vmax=18)\n",
    "        axes[i][1].set_title('Ground Truth Mask', fontsize=30)  # Larger font size (2x larger)\n",
    "        axes[i][1].axis('off')  # Turn off axis for the ground truth mask\n",
    "\n",
    "        # Convert output to predicted class indices\n",
    "        pred_mask = torch.argmax(outputs[i], dim=0).cpu()  # (512, 512) mask with class labels\n",
    "        axes[i][2].imshow(pred_mask, cmap='tab20', vmin=0, vmax=18)\n",
    "        axes[i][2].set_title('Predicted Mask', fontsize=30)  # Larger font size (2x larger)\n",
    "        axes[i][2].axis('off')  # Turn off axis for the predicted mask\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a few predictions\n",
    "visualize_predictions(model, val_loader, device, num_images=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Function to save a prediction mask as a PNG where pixel values are class labels\n",
    "def save_prediction(pred_mask, original_filename, output_dir='prediction'):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert the tensor to a NumPy array\n",
    "    pred_mask_np = pred_mask.cpu().numpy().astype(np.uint8)  # Ensure it's an 8-bit unsigned integer array\n",
    "    \n",
    "    # Save the NumPy array as a PNG image where pixel values are class labels\n",
    "    pred_img = Image.fromarray(pred_mask_np)\n",
    "    pred_img.save(os.path.join(output_dir, original_filename.replace('.jpg', '.png')))\n",
    "\n",
    "# Function to iterate through validation dataset and save predictions\n",
    "def save_predictions(model, val_loader, val_dataset, output_dir='prediction'):\n",
    "    model.eval()\n",
    "    model = model.cuda()\n",
    "\n",
    "    # Iterate over the validation data loader\n",
    "    for idx, (images, masks) in enumerate(val_loader):\n",
    "        images = images.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "\n",
    "        # Get the corresponding filenames for this batch\n",
    "        batch_filenames = val_dataset.images[idx * val_loader.batch_size : (idx + 1) * val_loader.batch_size]\n",
    "\n",
    "        for i in range(images.size(0)):\n",
    "            # Convert output to predicted class indices\n",
    "            pred_mask = torch.argmax(outputs[i], dim=0)  # (512, 512) mask with class labels\n",
    "\n",
    "            # Save the predicted mask with the original filename\n",
    "            original_filename = batch_filenames[i]\n",
    "            save_prediction(pred_mask, original_filename, output_dir)\n",
    "\n",
    "# Call the function to save predictions\n",
    "save_predictions(model, val_loader, val_dataset, output_dir=model_name+'prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def read_masks(path):\n",
    "    mask = Image.open(path)\n",
    "    mask = np.array(mask)\n",
    "    return mask\n",
    "\n",
    "\n",
    "# replace submit_dir to your result path here\n",
    "submit_dir = model_name + 'prediction'\n",
    "\n",
    "# replace truth_dir to ground-truth path here\n",
    "truth_dir = 'val/val_mask'\n",
    "\n",
    "# replace output_dir to the desired output path, and you will find 'scores.txt' containing the calculated mIoU\n",
    "output_dir = 'output'\n",
    "\n",
    "if not os.path.isdir(submit_dir):\n",
    "    print(f\"{submit_dir} doesn't exist\")\n",
    "\n",
    "if os.path.isdir(submit_dir) and os.path.isdir(truth_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    submit_dir_list = os.listdir(submit_dir)\n",
    "    if len(submit_dir_list) == 1:\n",
    "        submit_dir = os.path.join(submit_dir, f\"{submit_dir_list[0]}\")\n",
    "        assert os.path.isdir(submit_dir)\n",
    "\n",
    "    # Class names based on the label list provided\n",
    "    class_names = [\n",
    "        'background', 'skin', 'nose', 'eye_g', 'l_eye', 'r_eye', 'l_brow', 'r_brow', \n",
    "        'l_ear', 'r_ear', 'mouth', 'u_lip', 'l_lip', 'hair', 'hat', 'ear_r', \n",
    "        'neck_l', 'neck', 'cloth'\n",
    "    ]\n",
    "\n",
    "    area_intersect_all = np.zeros(19)\n",
    "    area_union_all = np.zeros(19)\n",
    "    \n",
    "    for idx in range(1000):\n",
    "        pred_mask = read_masks(os.path.join(submit_dir, f\"{idx}.png\"))\n",
    "        gt_mask = read_masks(os.path.join(truth_dir, f\"{idx}.png\"))\n",
    "        \n",
    "        for cls_idx in range(19):\n",
    "            area_intersect = np.sum(\n",
    "                (pred_mask == gt_mask) * (pred_mask == cls_idx))\n",
    "\n",
    "            area_pred_label = np.sum(pred_mask == cls_idx)\n",
    "            area_gt_label = np.sum(gt_mask == cls_idx)\n",
    "            area_union = area_pred_label + area_gt_label - area_intersect\n",
    "\n",
    "            area_intersect_all[cls_idx] += area_intersect\n",
    "            area_union_all[cls_idx] += area_union\n",
    "\n",
    "    iou_all = area_intersect_all / area_union_all * 100.0\n",
    "    miou = iou_all.mean()\n",
    "\n",
    "    # Create the evaluation score path for mIOU\n",
    "    output_filename = os.path.join(output_dir, model_name + 'scores.txt')\n",
    "    with open(output_filename, 'w') as f3:\n",
    "        f3.write(f'mIOU: {miou:.2f}%\\n')\n",
    "\n",
    "    # Write detailed IoU for each class to a separate file\n",
    "    detailed_output_filename = os.path.join(output_dir, model_name + 'detailed_scores.txt')\n",
    "    with open(detailed_output_filename, 'w') as f4:\n",
    "        f4.write('Class-wise IoU scores:\\n')\n",
    "        for cls_idx in range(19):\n",
    "            f4.write(f'{class_names[cls_idx]}: {iou_all[cls_idx]:.2f}%\\n')\n",
    "        f4.write(f'\\nMean IoU (mIOU): {miou:.2f}%\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !Test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "\n",
    "class SmallerUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallerUNet, self).__init__()\n",
    "\n",
    "        # Encoder: Convolution + ReLU + MaxPool (downsampling)\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # 512x512 -> 256x256\n",
    "        \n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # 256x256 -> 128x128\n",
    "        \n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # 128x128 -> 64x64\n",
    "        \n",
    "        self.encoder4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)  # 64x64 -> 32x32\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Decoder: Transpose Convolution + ReLU (upsampling)\n",
    "        self.upconv4 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)  # 32x32 -> 64x64\n",
    "        self.decoder4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)  # 64x64 -> 128x128\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)  # 128x128 -> 256x256\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)  # 256x256 -> 512x512\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Conv2d(16, 19, kernel_size=1)  # 1x1 Conv to map to the number of classes (19 in this case)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        enc1 = self.encoder1(x)\n",
    "        x = self.pool1(enc1)\n",
    "        \n",
    "        enc2 = self.encoder2(x)\n",
    "        x = self.pool2(enc2)\n",
    "        \n",
    "        enc3 = self.encoder3(x)\n",
    "        x = self.pool3(enc3)\n",
    "        \n",
    "        enc4 = self.encoder4(x)\n",
    "        x = self.pool4(enc4)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        # Decoder path with skip connections\n",
    "        x = self.upconv4(x)\n",
    "        x = torch.cat((enc4, x), dim=1)  # Concatenate encoder output (skip connection)\n",
    "        x = self.decoder4(x)\n",
    "\n",
    "        x = self.upconv3(x)\n",
    "        x = torch.cat((enc3, x), dim=1)\n",
    "        x = self.decoder3(x)\n",
    "\n",
    "        x = self.upconv2(x)\n",
    "        x = torch.cat((enc2, x), dim=1)\n",
    "        x = self.decoder2(x)\n",
    "\n",
    "        x = self.upconv1(x)\n",
    "        x = torch.cat((enc1, x), dim=1)\n",
    "        x = self.decoder1(x)\n",
    "\n",
    "        # Output layer\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "num_classes = 19  # Replace with the number of classes in your dataset\n",
    "\n",
    "# Specify a specific GPU (e.g., GPU 2)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the smaller model\n",
    "model = SmallerUNet()\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Print model summary using torchsummary (input size: 3x512x512 for RGB images)\n",
    "summary(model, input_size=(3, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F1\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "class TestTransform:\n",
    "    def __init__( \n",
    "        self, resize=(512, 512), mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)\n",
    "    ):\n",
    "        self.resize = resize\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # Resize the image\n",
    "        image = F1.resize(image, self.resize)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        image = F1.to_tensor(image)\n",
    "        \n",
    "        # Normalize the image\n",
    "        image = F1.normalize(image, mean=self.mean, std=self.std)\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as F1\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebAMaskHQTestDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_ext='jpg', transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.images = [file for file in os.listdir(image_dir) if file.endswith(f'.{image_ext}')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = F1.to_tensor(image)\n",
    "\n",
    "        return image, self.images[idx]  # Return the image and its filename for saving the results later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test transform to match val_transform parameters\n",
    "test_transform = TestTransform(\n",
    "    resize=(512, 512), \n",
    "    mean=(0.4914, 0.4822, 0.4465), \n",
    "    std=(0.2023, 0.1994, 0.2010)\n",
    ")\n",
    "\n",
    "# Use the transform when creating the test dataset\n",
    "test_dataset = CelebAMaskHQTestDataset(\n",
    "    image_dir='test_image',\n",
    "    image_ext='jpg',\n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "# Create DataLoader for the test set\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_name+'_best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Function to save a prediction mask as a PNG where pixel values are class labels\n",
    "def save_prediction(pred_mask, original_filename, output_dir='prediction'):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert the tensor to a NumPy array\n",
    "    pred_mask_np = pred_mask.cpu().numpy().astype(np.uint8)  # Ensure it's an 8-bit unsigned integer array\n",
    "    \n",
    "    # Save the NumPy array as a PNG image where pixel values are class labels\n",
    "    pred_img = Image.fromarray(pred_mask_np)\n",
    "    pred_img.save(os.path.join(output_dir, original_filename.replace('.jpg', '.png')))\n",
    "\n",
    "# Function to iterate through the test dataset and save predictions\n",
    "def save_test_predictions(model, test_loader, output_dir='prediction'):\n",
    "    model.eval()\n",
    "    model = model.to(device)  # Move the model to the specified device\n",
    "\n",
    "    # Iterate over the test data loader\n",
    "    for images, filenames in test_loader:  # Unpack images and their filenames\n",
    "        images = images.to(device)  # Move images to the specified device\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "\n",
    "        for i in range(images.size(0)):\n",
    "            # Convert output to predicted class indices\n",
    "            pred_mask = torch.argmax(outputs[i], dim=0)  # (512, 512) mask with class labels\n",
    "\n",
    "            # Save the predicted mask with the original filename\n",
    "            original_filename = filenames[i]\n",
    "            save_prediction(pred_mask, original_filename, output_dir)\n",
    "\n",
    "# Call the function to save test predictions\n",
    "save_test_predictions(model, test_loader, output_dir='./test_results/' + model_name + '_test_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_predictions(model, test_loader, output_dir='prediction', num_pairs=5):\n",
    "    model.eval()\n",
    "    model = model.to(device)  # Move the model to the specified device\n",
    "\n",
    "    fig, axes = plt.subplots(num_pairs, 2, figsize=(10, num_pairs * 5))\n",
    "\n",
    "    # Track the number of pairs visualized\n",
    "    count = 0\n",
    "\n",
    "    for images, filenames in test_loader:  # Unpack images and their filenames\n",
    "        images = images.to(device)  # Move images to the specified device\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "\n",
    "        for i in range(images.size(0)):\n",
    "            if count >= num_pairs:\n",
    "                break\n",
    "\n",
    "            # Convert output to predicted class indices\n",
    "            pred_mask = torch.argmax(outputs[i], dim=0).cpu().numpy().astype(np.uint8)  # (512, 512) mask with class labels\n",
    "\n",
    "            # Plot the original image\n",
    "            ax_image = axes[count, 0]\n",
    "            ax_image.imshow(images[i].cpu().permute(1, 2, 0).numpy())\n",
    "            ax_image.set_title('Original Image')\n",
    "            ax_image.axis('off')\n",
    "\n",
    "            # Plot the predicted mask\n",
    "            ax_mask = axes[count, 1]\n",
    "            ax_mask.imshow(pred_mask, cmap='viridis')  # Use a colour map to visualise the mask\n",
    "            ax_mask.set_title('Predicted Mask')\n",
    "            ax_mask.axis('off')\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        if count >= num_pairs:\n",
    "            break\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_label_histogram(model, test_loader):\n",
    "    model.eval()\n",
    "    model = model.to(device)  # Move the model to the specified device\n",
    "\n",
    "    label_counts = np.zeros(19, dtype=int)  # Assuming a maximum of 256 classes\n",
    "\n",
    "    for images, _ in test_loader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "\n",
    "        for i in range(images.size(0)):\n",
    "            pred_mask = torch.argmax(outputs[i], dim=0).cpu().numpy().astype(np.uint8)\n",
    "            unique, counts = np.unique(pred_mask, return_counts=True)\n",
    "            label_counts[unique] += counts\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(label_counts)), label_counts)\n",
    "    plt.xlabel('Class Label')\n",
    "    plt.ylabel('Pixel Count')\n",
    "    plt.title('Histogram of Predicted Labels')\n",
    "    plt.yscale('log')  # Use a log scale if there are large differences in counts\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize several image and predicted mask pairs\n",
    "visualize_predictions(model, test_loader, output_dir='./test_results/' + model_name + '_test_prediction', num_pairs=5)\n",
    "\n",
    "# Plot a histogram of predicted labels\n",
    "plot_label_histogram(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
